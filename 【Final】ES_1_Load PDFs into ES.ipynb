{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ElasticSearch -1: Load PDFs into ES**\n",
    "\n",
    "#### This notebook corresponds with the slide \"4.1. ES  -  Process  &  Bulk Load\"\n",
    "\n",
    "*------- All files relative paths need to be changed according to your file locations in order to run this notebook -------*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.Install ElasticSearch**\n",
    "1. **There are many ways to install ElaticSeach locally, for example:**\n",
    "   * Install Elasticsearch with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docker.html \n",
    "   * Install Elasticsearch on macOS with Homebrew: https://www.elastic.co/guide/en/elasticsearch/reference/7.16/brew.html#brew\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. **We use a method which is fast for testing purpose but not sophisiticated enough for development or production purposes:**\n",
    "   1. Following this article: https://medium.com/codex/all-you-need-to-know-about-using-elasticsearch-in-python-b9ed00e0fdf0  \n",
    "   2. Create a directory for ES and open it with VS Code\n",
    "   3. Install the Docker (by Microsoft) extension in VS Code\n",
    "   4. Create a \"docker-compose.yml\" file in the ES directory\n",
    "   5. Copy and paste the \"docker-compose.yml\" file content from the article to set up the  basic configurations for ES. Importantly, this file creates a persistent volume locally for Elasticsearch so that data can persist when the containers restart\n",
    "   6. Open terminal in VS Code and execute command \"docker-compose up -d\" to create a docker container for ES\n",
    "   7. Click the VS Code Docker tab, and right-click “elasticsearch:7.12.0” and “kibana:7.12.0” and select \"start\" for both\n",
    "   8. Use terminal command \"docker-compose ps\" or \"docker ps -a\" to check if this docker container is running, and use \"docker-compose logs -f\" to check logs of the services\n",
    "   9. Install ES Python client with Anaconda (https://anaconda.org/conda-forge/elasticsearch) or with Python (https://elasticsearch-py.readthedocs.io/en/v7.16.3/)\n",
    "   10. Import ES using \"from elasticsearch import Elasticsearch\", and open Kibana Console in web browser using URL \"http://localhost:5601/\"\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "3. **ES docs and examples**\n",
    "   * ES official doc: https://www.elastic.co/guide/index.html \n",
    "   * ES API Documentation: https://elasticsearch-py.readthedocs.io/en/7.x/api.html\n",
    "   * ES Quick Start: https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html  \n",
    "   * ES online book: https://livebook.manning.com/book/elasticsearch-in-action-second-edition/chapter-2/v-4 \n",
    "   * \"What is Elasticsearch and why is it so fast?\": https://lynn-kwong.medium.com/what-is-elasticsearch-and-why-is-it-so-fast-5a4b95747d19\n",
    "   * \"All you need to know about using Elasticsearch in Python\": https://medium.com/codex/all-you-need-to-know-about-using-elasticsearch-in-python-b9ed00e0fdf0\n",
    "   * \"Streaming structured data from Elasticsearch using Tensorflow-IO\": https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/elasticsearch.ipynb#scrollTo=xHxb-dlhMIzW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfFileReader\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import IndicesClient\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Load Files and Screen Raw PDFs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Issuer - subsidiary</th>\n",
       "      <th>Issuer industry</th>\n",
       "      <th>Country of risk</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Cagayan Electric Power &amp; Light Co Inc</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>2001-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16478</td>\n",
       "      <td>Nanjing Financial City Construction &amp; Developm...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>China</td>\n",
       "      <td>2021-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16481</td>\n",
       "      <td>Suzhou Tech City Development Group Co Ltd</td>\n",
       "      <td>Financials</td>\n",
       "      <td>China</td>\n",
       "      <td>2021-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16479</td>\n",
       "      <td>Landesbank Baden-Wuerttemberg</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2021-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16480</td>\n",
       "      <td>City of Lunds Sweden</td>\n",
       "      <td>Government</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>2021-04-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique ID                                Issuer - subsidiary  \\\n",
       "0          3              Cagayan Electric Power & Light Co Inc   \n",
       "1      16478  Nanjing Financial City Construction & Developm...   \n",
       "2      16481          Suzhou Tech City Development Group Co Ltd   \n",
       "3      16479                      Landesbank Baden-Wuerttemberg   \n",
       "4      16480                               City of Lunds Sweden   \n",
       "\n",
       "  Issuer industry Country of risk       Date  \n",
       "0          Energy     Philippines 2001-05-30  \n",
       "1      Financials           China 2021-04-30  \n",
       "2      Financials           China 2021-04-30  \n",
       "3      Financials         Germany 2021-04-30  \n",
       "4      Government          Sweden 2021-04-30  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### 3.1. Load the \"companies.xlsx\" file to enrich data, if multiple rows share the same \"Unique ID\",\n",
    "# then only use the first instance from this file\n",
    "df_com = pd.read_excel(\"Crawler & Processing/2.Develop - Crawler Folder/companies.xlsx\")\n",
    "df_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The number of positive raw PDF files is:  995\n",
      "--- The number of negative raw PDF files is:  1158\n",
      "--- The number of ALL raw PDF files is:  2153\n"
     ]
    }
   ],
   "source": [
    "##### 3.2. Load the paths of raw PDFs from the crawler and list out all raw PDF file names from the folders\n",
    "##### Separately process \"positive\" and \"negative\" reports to allow some flexibility for future works\n",
    "PDF_path_pos = \"Crawler & Processing/2.Develop - Crawler Folder/pos_reports/\"\n",
    "PDF_path_neg = \"Crawler & Processing/2.Develop - Crawler Folder/neg_reports/\"\n",
    "\n",
    "PDF_file_list_pos = os.listdir(PDF_path_pos)\n",
    "PDF_file_list_neg = os.listdir(PDF_path_neg)\n",
    "\n",
    "print(\"--- The number of positive raw PDF files is: \", len(PDF_file_list_pos))\n",
    "print(\"--- The number of negative raw PDF files is: \", len(PDF_file_list_neg))\n",
    "print(\"--- The number of ALL raw PDF files is: \", len(PDF_file_list_pos + PDF_file_list_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The number of postitive sustainability reports is:  456\n",
      "--- The number of negative sustainability reports is:  520\n",
      "--- The dataframe of positive sustainability results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Sustainability Report mention</th>\n",
       "      <th>first5sus</th>\n",
       "      <th>Annual Report mention</th>\n",
       "      <th>first5annual</th>\n",
       "      <th>exclusion</th>\n",
       "      <th>Year</th>\n",
       "      <th>bs_score</th>\n",
       "      <th>sustainability_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16357</td>\n",
       "      <td>387</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16447</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16423</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16456</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Company  Pages  Sustainability Report mention  first5sus  \\\n",
       "0           0       86      6                              1          1   \n",
       "1           1    16357    387                              1          1   \n",
       "2           2    16447     16                              1          1   \n",
       "3           3    16423    133                              1          1   \n",
       "4           4    16456     72                              1          1   \n",
       "\n",
       "   Annual Report mention  first5annual  exclusion  Year  bs_score  \\\n",
       "0                      0             0          1  2021         2   \n",
       "1                      1             1          0  2018         2   \n",
       "2                      0             0          1  2021         2   \n",
       "3                      1             0          0  2020         2   \n",
       "4                      1             0          0  2019         2   \n",
       "\n",
       "   sustainability_score  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The dataframe of negative sustainability results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Sustainability Report mention</th>\n",
       "      <th>first5sus</th>\n",
       "      <th>Annual Report mention</th>\n",
       "      <th>first5annual</th>\n",
       "      <th>exclusion</th>\n",
       "      <th>Year</th>\n",
       "      <th>bs_score</th>\n",
       "      <th>sustainability_score</th>\n",
       "      <th>Searched Year</th>\n",
       "      <th>Matching Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3982_2020</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15206_2020</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15194_2020</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3975_2020</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>15196_2020</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Company  Pages  Sustainability Report mention  first5sus  \\\n",
       "0           0   3982_2020     77                              1          1   \n",
       "1           1  15206_2020    285                              1          1   \n",
       "6           6  15194_2020     52                              1          1   \n",
       "7           7   3975_2020    156                              1          1   \n",
       "8           8  15196_2020    110                              1          1   \n",
       "\n",
       "   Annual Report mention  first5annual  exclusion  Year  bs_score  \\\n",
       "0                      1             0          0  2019         2   \n",
       "1                      1             0          0  2020         2   \n",
       "6                      1             0          0  2020         2   \n",
       "7                      1             1          0  2020         2   \n",
       "8                      1             0          1  2020         2   \n",
       "\n",
       "   sustainability_score  Searched Year  Matching Year  \n",
       "0                     0           2020          False  \n",
       "1                     0           2020           True  \n",
       "6                     0           2020           True  \n",
       "7                     0           2020           True  \n",
       "8                     0           2020           True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### 3.3. Load the results from the BS detector \n",
    "\n",
    "# Load the results(CSVs) from the BS Detector into dataframes\n",
    "df_pos = pd.read_csv(\"BS Detector Results csv/positives_bsdetector.csv\")\n",
    "df_neg = pd.read_csv(\"BS Detector Results csv/negatives_bsdetector.csv\")\n",
    "\n",
    "# Select ONLY the sustainability reports (bs_score=2)\n",
    "df_pos_sus = df_pos[df_pos[\"bs_score\"] == 2]\n",
    "df_neg_sus = df_neg[df_neg[\"bs_score\"] == 2]\n",
    "print(\"--- The number of postitive sustainability reports is: \", df_pos_sus.shape[0])\n",
    "print(\"--- The number of negative sustainability reports is: \", df_neg_sus.shape[0])\n",
    "\n",
    "print(\"--- The dataframe of positive sustainability results: \")\n",
    "display(df_pos_sus.head())\n",
    "print(\"--- The dataframe of negative sustainability results: \")\n",
    "display(df_neg_sus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.4. Define a function to screen out unreadable PDFs by PdfFileReader\n",
    "def check_readable_files(df_sus, file_path, label):\n",
    "    \"\"\"[summary]\n",
    "    This function iterates through all raw PDFs, reads them and counts the pages, and puts results into  \n",
    "    \"readable_fileID_list\" and \"page_nr_list\" to screen out unreadable PDFs by PdfFileReader\n",
    "    \n",
    "    Args:\n",
    "        df_sus ([dataframe]): dataframe from the BS detector as loaded 1 cell above\n",
    "        file_path ([string]): paths of raw PDFs \n",
    "        label ([string]): \"positive\" or \"negative\"\n",
    "\n",
    "    Returns:\n",
    "        readable_fileID_list ([list]): a list of readable file names (str)\n",
    "        page_nr_list ([list]): a list of total page number of each readable file (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check total page number of all sustainability reports using BS detector result dataframe\n",
    "    df_page_nr = df_sus[\"Pages\"].sum()\n",
    "\n",
    "    # Load total page number of all raw PDF files \n",
    "    readable_fileID_list = []\n",
    "    page_nr_list = []\n",
    "    \n",
    "    # Go through each raw PDF and append each file name and total page number to the lists above\n",
    "    for pdf_ID in df_sus[\"Company\"].values:\n",
    "        try:\n",
    "            fileName = str(pdf_ID)+\".pdf\"\n",
    "            if os.path.isfile(os.path.join(file_path, fileName)):\n",
    "                with open(file_path + fileName, 'rb') as f:\n",
    "                    pdf = PdfFileReader(f)\n",
    "                    page_nr = pdf.getNumPages()\n",
    "                    page_nr_list.append(page_nr)\n",
    "                    readable_fileID_list.append(fileName)\n",
    "        except:\n",
    "                continue\n",
    "    \n",
    "    # Print comparison information\n",
    "    print(\"--- Total number of {} sustainability reports from the BS Detector dataframe: {}\".format(label, df_sus.shape[0]))\n",
    "    print(\"--- Total page number of {} reports from the BS Detector dataframe: {}\".format(label, df_page_nr))\n",
    "    print(\"--- Total READABLE number of {} raw PDFs: {}\".format(label, len(readable_fileID_list)))\n",
    "    print(\"--- Total READABLE page number of {} raw PDFs: {}\".format(label, sum(page_nr_list)))\n",
    "    print(\"--- The number of reports dropped due to readability issue: \", df_sus.shape[0] - len(readable_fileID_list))\n",
    "    print(\"--- The number of pages dropped due to readability issue: \", df_page_nr - sum(page_nr_list))\n",
    "    \n",
    "    # Return READABLE file name list and page number list for further use\n",
    "    return readable_fileID_list, page_nr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total number of positive sustainability reports from the BS Detector dataframe: 456\n",
      "--- Total page number of positive reports from the BS Detector dataframe: 48158\n",
      "--- Total READABLE number of positive raw PDFs: 432\n",
      "--- Total READABLE page number of positive raw PDFs: 45430\n",
      "--- The number of reports dropped due to readability issue:  24\n",
      "--- The number of pages dropped due to readability issue:  2728\n"
     ]
    }
   ],
   "source": [
    "##### 3.5. Use the function above to screen readable positive PDFs by PdfFileReader\n",
    "# and get 2 variables to be used for bulk processing data for ES later\n",
    "readable_fileID_list_pos, page_nr_list_pos = check_readable_files(df_sus = df_pos_sus,\n",
    "                                                                  file_path = PDF_path_pos,\n",
    "                                                                  label = \"positive\")\n",
    "#【Time of running this cell: 1min】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total number of negative sustainability reports from the BS Detector dataframe: 520\n",
      "--- Total page number of negative reports from the BS Detector dataframe: 55649\n",
      "--- Total READABLE number of negative raw PDFs: 492\n",
      "--- Total READABLE page number of negative raw PDFs: 51608\n",
      "--- The number of reports dropped due to readability issue:  28\n",
      "--- The number of pages dropped due to readability issue:  4041\n"
     ]
    }
   ],
   "source": [
    "##### 3.6. Use the function above to screen readable negative PDFs by PdfFileReader\n",
    "# and get 2 variables to be used for bulk processing data for ES later\n",
    "readable_fileID_list_neg, page_nr_list_neg = check_readable_files(df_sus = df_neg_sus,\n",
    "                                                                  file_path = PDF_path_neg,\n",
    "                                                                  label = \"negative\")\n",
    "#【Time of running this cell: 1min】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Prepare Text Embedding and Functions for ES Bulk Loading**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Download the USE Encoder to Generate Text Embedding Vectors for Semantic Search\n",
    "\n",
    "The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity and clustering.  USE has two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN). We use Transformer version, since it generally has higher accuracy. We create vector matrices of text embeddings of our reference data, then we get the vectors of the embeddings of text from each report page and then compare the similarity scores. USE is element-wise sum of context-aware word representations, like BERT, but USE is particularly trained to identify the semantic similarity between sentences.\n",
    "\n",
    "* Useful Links:\n",
    "    * TensorFlow Hub: https://tfhub.dev/\n",
    "    * universal-sentence-encoder: https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
    "    * \"Review-based Search Engine using Universal Sentence Encoder and PEGASUS\": https://medium.com/@peacelikejoy/review-based-search-engine-using-universal-sentence-encoder-and-pegasus-3643d6456b9f \n",
    "    * \"Vector-Based Semantic Search using Elasticsearch\": https://medium.com/version-1/vector-based-semantic-search-using-elasticsearch-48d7167b38f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Downloading pre-trained embeddings from tensorflow hub…\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done.\n",
      "--- Creating tensorflow session…\n",
      "--- Done.\n"
     ]
    }
   ],
   "source": [
    "# Import the essential TensorFlow libraries：\n",
    "import tensorflow.compat.v1 as tf \n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Download the Universal Sentence Encoder Model (about 1 GB)：\n",
    "# Since TF ver.1 is used here, pretrained TF embedding modules need to be loaded with \"hub.Module\"\n",
    "# With TF ver.2, \"hub.load()\" is used instead to handle valid SavedModel, instead of modules\n",
    "# About USE: Universal-Sentence-Encoder https://tfhub.dev/google/universal-sentence-encoder/4 \n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    print(\"--- Downloading pre-trained embeddings from tensorflow hub…\")\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\") \n",
    "    text_ph = tf.placeholder(tf.string)\n",
    "    embeddings_1 = embed(text_ph)\n",
    "    print(\"--- Done.\")\n",
    "    print(\"--- Creating tensorflow session…\")\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(\"--- Done.\")\n",
    "\n",
    "#【Time of running this cell the first time: 20min】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Text to be embedded: We show our efforts to help the green economy, creating business and value by recycling plastic waste. We focus on the positive impact on the environment and people through further growing our sustainable offering. We create technologies and solutions to advance a more efficient, sustainable, resilient and environment-friendly world for all. We perform detailed analysis to evaluate the significance of working activities that influence the environment. Our Environmental policy is also defined in the engineering phase which is an opportunity to propose technological modifications which can result in energy saving and cleaner emissions, leading to environmental benefits for the customer, stakeholders and the whole community. We are using only renewable energy. All our electricity is from renewable sources. Our electricity mainly come from solar panels and wind power. We demonstrate our commitment to this policy by striving to ensure that our actions have no or minimal impact on our planet. We have reduced our green house gas emissions. We are committed to promote decarbonization and better use of energy, continuously implement energy efficiency initiatives. Water consumption has been reduced and water has been recycled with innovative technologies.  It’s essential to protect water, not only for our business needs, but also for the sake of the communities in which we operate, because access to clean, fresh water is a critical human need. We also implemented a comprehensive water management system that includes a rainwater harvesting system. We have undertaken careful and comprehensive collection, transportation and final treatment of waste. Our digitalization of documents assists a paper-less approach which helps to reduce paper waste. We have successfully used innovative technologies to minimize hazard wastes. Negative impact on the environment has been reduced. During each audit we inspect environmental permits, waste management, and effluent treatment plants. We began an office eco-efficiency program aimed at reduction, reuse and recycling of waste. Each office has designated recycling bins. We have eliminated plastic from our packaging. We also committed to a plastic-free future. We have reduced carbon (CO2 ) emissions and reduced our carbon footprint of our operations, products and services. We achieved net-zero operational emissions. Circularity is part of our business model and we are expanding our environmental commitments to integrate biodiversity. We have started a series of initiatives to protect animal and plants biodiversity. We have been actively source sustainable green materials during our production. \n",
      "\n",
      "--- Embedding size: 512 \n",
      "\n",
      "--- Obtained Embedding[[-0.05246104300022125, 0.04983028396964073, -0.0017246127827093005, 0.056376032531261444, 0.04131313040852547],…]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to use the USE encoder to convert texts to embedding vectors:\n",
    "def text_to_vector(text):\n",
    "    vectors = session.run(embeddings_1, feed_dict={text_ph: text})\n",
    "    return [vector.tolist() for vector in vectors]\n",
    "\n",
    "# Define a testing reference text and call the \"text_to_vector\" function to convert：\n",
    "testing_text = \"We show our efforts to help the green economy, creating business and value by recycling plastic waste. We focus on the positive impact on the environment and people through further growing our sustainable offering. We create technologies and solutions to advance a more efficient, sustainable, resilient and environment-friendly world for all. We perform detailed analysis to evaluate the significance of working activities that influence the environment. Our Environmental policy is also defined in the engineering phase which is an opportunity to propose technological modifications which can result in energy saving and cleaner emissions, leading to environmental benefits for the customer, stakeholders and the whole community. We are using only renewable energy. All our electricity is from renewable sources. Our electricity mainly come from solar panels and wind power. We demonstrate our commitment to this policy by striving to ensure that our actions have no or minimal impact on our planet. We have reduced our green house gas emissions. We are committed to promote decarbonization and better use of energy, continuously implement energy efficiency initiatives. Water consumption has been reduced and water has been recycled with innovative technologies.  It’s essential to protect water, not only for our business needs, but also for the sake of the communities in which we operate, because access to clean, fresh water is a critical human need. We also implemented a comprehensive water management system that includes a rainwater harvesting system. We have undertaken careful and comprehensive collection, transportation and final treatment of waste. Our digitalization of documents assists a paper-less approach which helps to reduce paper waste. We have successfully used innovative technologies to minimize hazard wastes. Negative impact on the environment has been reduced. During each audit we inspect environmental permits, waste management, and effluent treatment plants. We began an office eco-efficiency program aimed at reduction, reuse and recycling of waste. Each office has designated recycling bins. We have eliminated plastic from our packaging. We also committed to a plastic-free future. We have reduced carbon (CO2 ) emissions and reduced our carbon footprint of our operations, products and services. We achieved net-zero operational emissions. Circularity is part of our business model and we are expanding our environmental commitments to integrate biodiversity. We have started a series of initiatives to protect animal and plants biodiversity. We have been actively source sustainable green materials during our production.\"\n",
    "text_vector = text_to_vector([testing_text])[0]\n",
    "print(\"--- Text to be embedded: {}\".format(testing_text), \"\\n\")\n",
    "print(\"--- Embedding size: {}\".format(len(text_vector)), \"\\n\")\n",
    "print(\"--- Obtained Embedding[{},…]\\n\".format(text_vector[:5]))\n",
    "\n",
    "#【Time of running this cell the first time: 1sec】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Load ES Client and Define ES Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ES client for ES operations\n",
    "es_client = Elasticsearch(\"localhost:9200\",  # Host here is a single node. Most of the time we only connect to a single node for testing purpose\n",
    "                          http_auth=[\"elastic\", \"ING_project\"],  # Default port, user name, and self-defined password\n",
    "                          timeout=300)  #【!】Set \"timeout\" parameter to allow longer data loading/indexing time\n",
    "\n",
    "# Create an ES index client to work with indexes\n",
    "es_index_client = IndicesClient(es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Settings & Mappings of ES [[[[[-------IMPORTANT!--------]]]]]\n",
    "# In practical usage, WE always define settings and mappings which can make ES engine more robust, efficient and powerful\n",
    "configurations = {\n",
    "    \"settings\": {  # Setting part of the index\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": 1}, # Make no difference in a local environment, but in production multiple replicas \n",
    "                                      # can improve availability and fault tolerance\n",
    "        \"max_result_window\" : 100000,  #【!】Set larger than our total doc/page number, which is about 96000+, otherwise python\n",
    "                                       # won't be able to retrive all results > then default size 10,000\n",
    "        \"analysis\": { # Define an ngram filter and analyzer here which supports searching by partial input or autocompletion\n",
    "            \"filter\": {\n",
    "                \"ngram_filter\": {\n",
    "                  \"type\": \"edge_ngram\",\n",
    "                  \"min_gram\": 2,\n",
    "                  \"max_gram\": 512}\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"ngram_analyzer\": { # Define parameters for the ngram_analyzer analyzer \n",
    "                                    # \"An Introduction to Ngrams in Elasticsearch\": https://qbox.io/blog/an-introduction-to-ngrams-in-elasticsearch\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                      \"lowercase\",\n",
    "                      \"ngram_filter\"]\n",
    "                }  \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": { # Data schema of the index. ES supports dynamic mapping, which means we don’t need to define the field types \n",
    "                  # in advance and Elasticsearch will create them automatically. However, we should always define the mapping whenever possible. \n",
    "                  # It is better to be explicit about the mapping than implicit. The more you know about your data, the more robust the ES engine can be.\n",
    "        \"dynamic\": \"true\",\n",
    "        \"_source\": {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"id\": {\n",
    "                \"type\": \"text\", # Set doc id to be strings to include both \"doc id\" and \"page number\", like \"86.1\"\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"long\", # Positive label = 1, Negative label = 0\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"company\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": { # Add \"keyword\" field to allow aggregation and advanced seach\n",
    "                        \"type\": \"keyword\"} \n",
    "                }\n",
    "            },\n",
    "            \"industry\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"date\": {  # Set the date to be numerical type to allow range search\n",
    "                \"type\": \"long\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"filename\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"},\n",
    "                    \"ngrams\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"ngram_analyzer\"}\n",
    "                }\n",
    "            },\n",
    "            \"page\": { # This page numebr = origiral/actual page number - 1 due to PdfFileReader's non-zero indexing\n",
    "                \"type\": \"long\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"text_len\": { # Number of words contained in the \"text\" field of this doc/page\n",
    "                \"type\": \"long\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"}\n",
    "                }\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"},\n",
    "                    \"ngrams\": { # Allow this field to use the ngram_analyzer analyzer\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"ngram_analyzer\"}\n",
    "                }\n",
    "            },\n",
    "            \"emb_text_vector\": { \n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 512  #【!】Needs to be the same as the dimenstion of USE-encoded text embedding vectors\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Define a Function to Extract Texts from PDF pages with Enriched data for ES Bulk Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read pages from each PDF into action_lists \n",
    "def pdf_page_to_action(file_list, file_path, start, end, index_name, label, json_name):\n",
    "    \"\"\"\n",
    "    [summary]\n",
    "    This function iterates through each raw PDF from the given file_path, extracts the file name to get the PDF id,\n",
    "    splits files into small batches according to the \"start\" and \"end\" index to avoid ES data overload,\n",
    "    uses the PDF id to match the company information, extracts texts from PDFs, creates indexing actions using \n",
    "    the index_name, creates ES doc ids using \"PDF id + page number\" strings, creates \"docs\" as the main data\n",
    "    bodies that contain {id, company info, file and page info, texts and text embedding vectors}, then appends the action\n",
    "    and the corresponding doc body into the action_list, and finally generates json files using each action_list\n",
    "    for backup purpose\n",
    "    \n",
    "    Args:\n",
    "        file_list ([list]): the previously generated readable PDF lists, such as \"readable_fileID_list_pos\"\n",
    "        file_path ([string]): the raw PDF file path defined in previous cell\n",
    "        start ([int]): start index of the small batch\n",
    "        end ([int]): end index of the small batch\n",
    "        index_name ([string]): index name for ES indexing\n",
    "        label ([bool number]): positive=1, negative=0\n",
    "        json_name ([string]): json file name\n",
    "\n",
    "    Returns:\n",
    "        action_list ([string]): a list of \"action & doc body\" pairs for ES bulk API to load data into ES\n",
    "    \"\"\"\n",
    "    \n",
    "    action_list = []  \n",
    "\n",
    "    # Get file names from the file_list indexed by \"start\" and \"end\" points\n",
    "    for fileName in file_list[start : end]:\n",
    "        try:\n",
    "            # Get PDF id before \".csv\" for positive PDFs and before \"_year.csv\" for negative PDFs\n",
    "            if label== 1: \n",
    "                pdf_ID = fileName.split(\".\")[0] \n",
    "            else:\n",
    "                pdf_ID = fileName.split(\"_\")[0]\n",
    "                \n",
    "            # Match company info and only use the first instance from this df_com from previous cell\n",
    "            matched_row = df_com[df_com[\"Unique ID\"]== int(pdf_ID)].iloc[0] \n",
    "            \n",
    "            # Open raw PDFs and extract text with enriched data, all mapped according to ES mappings\n",
    "            with open(file_path + fileName, 'rb') as f:\n",
    "                pdf = PdfFileReader(f)\n",
    "                for pn in range(1, pdf.getNumPages()): # Get all pages of a PDF, has to start from \"1\"\n",
    "                    doc_id_str = str(pdf_ID)+\".\"+str(pn) # Create ES doc id = \"PDF id + page number\" string\n",
    "                    page = pdf.getPage(pn)   # Retrieves a page by number from this PDF file\n",
    "                    text = page.extractText()   # Extract the text on this page\n",
    "                    text_vector = text_to_vector([text])[0] # Call the function to convert text into text embedding vectors\n",
    "                    \n",
    "                    # Create an action for each page with an unique ES doc id\n",
    "                    action = {\"index\": {\"_index\": index_name, \"_id\": doc_id_str}}\n",
    "                    # Create an data body in a doc Dict corresponding to each page/doc\n",
    "                    doc = { \n",
    "                        \"id\": doc_id_str,\n",
    "                        \"label\": label,\n",
    "                        \"company\": matched_row[\"Issuer - subsidiary\"],\n",
    "                        \"industry\": matched_row[\"Issuer industry\"],\n",
    "                        \"country\": matched_row[\"Country of risk\"],\n",
    "                        \"date\": int(str(matched_row[\"Date\"]).split(\"-\")[0]), \n",
    "                        \"filename\": fileName,\n",
    "                        \"page\": pn,\n",
    "                        \"text_len\":len(text.split(\" \")), \n",
    "                        \"text\": text,\n",
    "                        \"emb_text_vector\": text_vector\n",
    "                    }   \n",
    "                    action_list.append(json.dumps(action))  \n",
    "                    action_list.append(json.dumps(doc)) \n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    # Feed \"action_list\" into a JSON File for convenient access and as backups\n",
    "    with open(\"{}.json\".format(json_name), \"w\") as write_file:\n",
    "        write_file.write(\"\\n\".join(action_list))\n",
    "        \n",
    "    return action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Create Action Lists and Bulk Load Data into ES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing positive reports ---\n",
      "action_list_1  is created for sus_reports_1\n",
      "action_list_2  is created for sus_reports_2\n",
      "action_list_3  is created for sus_reports_3\n",
      "action_list_4  is created for sus_reports_4\n",
      "action_list_5  is created for sus_reports_5\n",
      "action_list_6  is created for sus_reports_6\n",
      "action_list_7  is created for sus_reports_7\n",
      "action_list_8  is created for sus_reports_8\n",
      "action_list_9  is created for sus_reports_9\n",
      "action_list_10  is created for sus_reports_10\n",
      "action_list_11  is created for sus_reports_11\n",
      "action_list_12  is created for sus_reports_12\n",
      "action_list_13  is created for sus_reports_13\n",
      "action_list_14  is created for sus_reports_14\n",
      "action_list_15  is created for sus_reports_15\n",
      "action_list_16  is created for sus_reports_16\n",
      "action_list_17  is created for sus_reports_17\n",
      "action_list_18  is created for sus_reports_18\n",
      "action_list_19  is created for sus_reports_19\n",
      "action_list_20  is created for sus_reports_20\n",
      "action_list_21  is created for sus_reports_21\n",
      "action_list_22  is created for sus_reports_22\n",
      "action_list_23  is created for sus_reports_23\n",
      "action_list_24  is created for sus_reports_24\n",
      "action_list_25  is created for sus_reports_25\n",
      "action_list_26  is created for sus_reports_26\n",
      "action_list_27  is created for sus_reports_27\n",
      "action_list_28  is created for sus_reports_28\n",
      "action_list_29  is created for sus_reports_29\n",
      "action_list_30  is created for sus_reports_30\n",
      "action_list_31  is created for sus_reports_31\n",
      "action_list_32  is created for sus_reports_32\n",
      "action_list_33  is created for sus_reports_33\n",
      "action_list_34  is created for sus_reports_34\n",
      "action_list_35  is created for sus_reports_35\n",
      "action_list_36  is created for sus_reports_36\n",
      "action_list_37  is created for sus_reports_37\n",
      "action_list_38  is created for sus_reports_38\n",
      "action_list_39  is created for sus_reports_39\n",
      "action_list_40  is created for sus_reports_40\n",
      "action_list_41  is created for sus_reports_41\n",
      "action_list_42  is created for sus_reports_42\n",
      "action_list_43  is created for sus_reports_43\n",
      "action_list_44  is created for sus_reports_44\n",
      "--- Processing negative reports ---\n",
      "action_list_45  is created for sus_reports_45\n",
      "action_list_46  is created for sus_reports_46\n",
      "action_list_47  is created for sus_reports_47\n",
      "action_list_48  is created for sus_reports_48\n",
      "action_list_49  is created for sus_reports_49\n",
      "action_list_50  is created for sus_reports_50\n",
      "action_list_51  is created for sus_reports_51\n",
      "action_list_52  is created for sus_reports_52\n",
      "action_list_53  is created for sus_reports_53\n",
      "action_list_54  is created for sus_reports_54\n",
      "action_list_55  is created for sus_reports_55\n",
      "action_list_56  is created for sus_reports_56\n",
      "action_list_57  is created for sus_reports_57\n",
      "action_list_58  is created for sus_reports_58\n",
      "action_list_59  is created for sus_reports_59\n",
      "action_list_60  is created for sus_reports_60\n",
      "action_list_61  is created for sus_reports_61\n",
      "action_list_62  is created for sus_reports_62\n",
      "action_list_63  is created for sus_reports_63\n",
      "action_list_64  is created for sus_reports_64\n",
      "action_list_65  is created for sus_reports_65\n",
      "action_list_66  is created for sus_reports_66\n",
      "action_list_67  is created for sus_reports_67\n",
      "action_list_68  is created for sus_reports_68\n",
      "action_list_69  is created for sus_reports_69\n",
      "action_list_70  is created for sus_reports_70\n",
      "action_list_71  is created for sus_reports_71\n",
      "action_list_72  is created for sus_reports_72\n",
      "action_list_73  is created for sus_reports_73\n",
      "action_list_74  is created for sus_reports_74\n",
      "action_list_75  is created for sus_reports_75\n",
      "action_list_76  is created for sus_reports_76\n",
      "action_list_77  is created for sus_reports_77\n",
      "action_list_78  is created for sus_reports_78\n",
      "action_list_79  is created for sus_reports_79\n",
      "action_list_80  is created for sus_reports_80\n",
      "action_list_81  is created for sus_reports_81\n",
      "action_list_82  is created for sus_reports_82\n",
      "action_list_83  is created for sus_reports_83\n",
      "action_list_84  is created for sus_reports_84\n",
      "action_list_85  is created for sus_reports_85\n",
      "action_list_86  is created for sus_reports_86\n",
      "action_list_87  is created for sus_reports_87\n",
      "action_list_88  is created for sus_reports_88\n",
      "action_list_89  is created for sus_reports_89\n",
      "action_list_90  is created for sus_reports_90\n",
      "action_list_91  is created for sus_reports_91\n",
      "action_list_92  is created for sus_reports_92\n",
      "action_list_93  is created for sus_reports_93\n",
      "action_list_94  is created for sus_reports_94\n",
      "--- Done! 94 action lists are created in total! ---\n"
     ]
    }
   ],
   "source": [
    "##### 5.1. Call the \"pdf_page_to_action\" function and create action lists for all pages from all raw PDFs\n",
    "# Define placeholders Dicts and the initial values (\"start\" and \"end\") used to split PDFs into batches\n",
    "action_dict_POS = {}\n",
    "action_dict_NEG = {}\n",
    "start = 0\n",
    "interval = 10 # Determins the number of PDFs in each batch\n",
    "counter = 1\n",
    "\n",
    "# Processing positive reports into batches\n",
    "print(\"--- Processing positive reports ---\")\n",
    "while start < len(readable_fileID_list_pos):\n",
    "    # Define the end point for each interation\n",
    "    if start + interval > len(readable_fileID_list_pos):\n",
    "        end = len(readable_fileID_list_pos) \n",
    "    else:\n",
    "        end = start + interval\n",
    "    # Create an action_list using the function defined above\n",
    "    action_list = pdf_page_to_action(file_list=readable_fileID_list_pos, # \"readable_fileID_list_pos\" defined ealier\n",
    "                                     file_path=PDF_path_pos, # \"PDF_path_pos\" defined ealier\n",
    "                                     start=start, # changes in every iteration\n",
    "                                     end=end, # changes in every iteration\n",
    "                                     label=1, # positive label = 1\n",
    "                                     index_name=\"sus_reports_\" + str(counter), # dynamically generate index names using the counter\n",
    "                                     json_name=\"sus_reports_\" + str(counter) \n",
    "                                     )\n",
    "    # Append this action_list into action_dict_POS Dict\n",
    "    key = \"action_list_\" + str(counter)\n",
    "    action_dict_POS[key] = action_list\n",
    "    print(\"action_list_\" + str(counter), \" is created for sus_reports_\" + str(counter))\n",
    "    # Increase the \"start\" number by \"interval\", an increase \"counter\" by 1\n",
    "    start += interval\n",
    "    counter += 1\n",
    "\n",
    "# Reset the splitters but use the last \"counter\" value\n",
    "# 【!】to ensure the index name has a continuous \"sus_reports_*\" pattern for bulk searching later!!!!!\n",
    "start = 0\n",
    "interval = 10\n",
    "# Processing negative reports into batches, same logic\n",
    "print(\"--- Processing negative reports ---\")\n",
    "while start < len(readable_fileID_list_neg):\n",
    "    if start + interval > len(readable_fileID_list_neg):\n",
    "        end = len(readable_fileID_list_neg) \n",
    "    else:\n",
    "        end = start + interval\n",
    "    action_list = pdf_page_to_action(file_list=readable_fileID_list_neg, \n",
    "                                     file_path=PDF_path_neg, \n",
    "                                     start=start, \n",
    "                                     end=end, \n",
    "                                     label=0, \n",
    "                                     index_name=\"sus_reports_\" + str(counter), \n",
    "                                     json_name=\"sus_reports_\" + str(counter)\n",
    "                                     )\n",
    "    key = \"action_list_\" + str(counter)\n",
    "    action_dict_NEG[key] = action_list\n",
    "    print(\"action_list_\" + str(counter), \" is created for sus_reports_\" + str(counter))\n",
    "    start += interval\n",
    "    counter += 1\n",
    "    \n",
    "# Print out ending message with total number of action lists (AKA: indexes)\n",
    "print(\"--- Done! {} action lists are created in total! ---\".format(len(action_dict_POS)+len(action_dict_NEG)))\n",
    "\n",
    "#【Time of running this cell: 40min for pos files + 45min for neg file = 80~90min】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5.2. Bulk load all action lists into ES\n",
    "#【!】Iterate through each action list as separate indexes and load data from \"doc\" body into ES as separate docs \n",
    "# to keep each index's data quantity below ES data size limit\n",
    "\n",
    "# Iterate through action_dicts\n",
    "for action_dict in [action_dict_POS, action_dict_NEG]:\n",
    "    # Iterate through each key-value pair in each action_dict\n",
    "    for key, value in action_dict.items():\n",
    "        # Create index names from action lists\n",
    "        index_name = \"sus_reports_\"+key.split(\"_\")[-1] \n",
    "        # If an index already exists in ES then delete it \n",
    "        if es_client.indices.exists(index=index_name):  \n",
    "            es_client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "        # Create a new index\n",
    "        es_index_client.create(index=index_name, \n",
    "                               settings=configurations[\"settings\"], \n",
    "                               mappings=configurations[\"mappings\"], \n",
    "                               request_timeout=1000) # 【!】Set a large number to avoid ES \"Request Timeout\" problems\n",
    "        # Bulk load data from this action list to the corresponding index in ES\n",
    "        es_client.bulk(body=\"\\n\".join(value)) \n",
    "\n",
    "#【Time of running this cell: 1~2min !!!】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cb/3qy8wy6s2571t48bzrjk5vl80000gn/T/ipykernel_73982/4243281523.py:12: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  result = es_client.search(index=\"sus_reports_*\", body=search_query, request_timeout=1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total number of indexes retrived:  94\n",
      "--- Total number of docs/pages retrived:  93950\n",
      "--- Total words stored in ES:  29215337\n"
     ]
    }
   ],
   "source": [
    "#### 5.3. Search for all docs to check the total number of docs/pages loaded into ES\n",
    "search_query = {\n",
    "    \"size\": 100000, \n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "        },\n",
    "    \"_source\": [\"id\",\"label\", \"company\", \"industry\", \"country\", \"date\",\"filename\", \"page\", \"text_len\"] \n",
    "    # Need to specify a few fields to limit the size of data retrieved to avoid ES crash\n",
    "}\n",
    "\n",
    "# Get search results and show basic stats\n",
    "result = es_client.search(index=\"sus_reports_*\", body=search_query, request_timeout=1000)\n",
    "Total_index_count = result[\"_shards\"][\"total\"]\n",
    "print(\"--- Total number of indexes retrived: \", Total_index_count)\n",
    "\n",
    "Total_page_count = len(result[\"hits\"][\"hits\"])\n",
    "print(\"--- Total number of docs/pages retrived: \", Total_page_count)\n",
    "\n",
    "Total_word_count = 0\n",
    "for dict in result[\"hits\"][\"hits\"]:\n",
    "    text_len = dict[\"_source\"][\"text_len\"]\n",
    "    Total_word_count += text_len\n",
    "print(\"--- Total words stored in ES: \", Total_word_count)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89e63de8770f628c77675a9211c73c50d8048a4fbbbf8de889960f40f43afefe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
